#!/usr/bin/env python3

import os
import re
import sys
import math
import time
import shutil
import datetime
import functools
import subprocess
import http.server
import urllib.error
import urllib.request
import multiprocessing.dummy

import git


SITE_URL = 'https://eyqs.ca'
SITE_TITLE = 'Eugene Y. Q. Shen'
REPO_REMOTE = 'origin'
BUILD_BRANCH = 'gh-pages'
SERVER_PORT = 8080

REPO_ROOT = '/home/eugeneyqshen/web'
BUILD_DIR = '_site'
LAYOUT_DIR = '_layouts'
INCLUDE_DIR = '_includes'
BLOG_DIR = 'blog'
ASSETS_DIR = 'assets'
INDEX_FILE = 'index.html'
SCRIPT_FILE = 'web'
BUILD_PATH = os.path.join(REPO_ROOT, BUILD_DIR)

PREAMBLE_SEPARATOR = '---\n'
ARCHIVE_URL = 'http://web.archive.org/save'
COPIED_NAMES = ['.nojekyll']
IGNORED_NAMES = [BUILD_DIR, ASSETS_DIR, LAYOUT_DIR, INCLUDE_DIR,
        SCRIPT_FILE, 'LICENSE.md', 'README.md']
CONTENT_EXTENSIONS = ['.html', '.md']

LAYOUT_KEY = 'layout'
CONTENT_TAG = 'content'
INCLUDE_TAG = 'include'
SPECIAL_KEYS = [CONTENT_TAG, INCLUDE_TAG,
        'url', 'site_title', 'head_title', 'post_date']



def get_name(name):
    return os.path.splitext(name)[0]

def get_extension(name):
    return os.path.splitext(name)[1]

def strip_repo_root(path):
    return os.path.relpath(path, REPO_ROOT)

def is_ignored(name):
    return ((name in IGNORED_NAMES or name.startswith('.'))
            and name not in COPIED_NAMES)

def is_content(name):
    return get_extension(name) in CONTENT_EXTENSIONS

def has_tag(line):
    return '{{' in line and '}}' in line

def get_tag(line):
    start = line.index('{{')
    end = line.index('}}')
    return line[start+2:end].strip()

def replace_tag(line, value):
    start = line.index('{{')
    end = line.index('}}')
    return line.replace(line[start:end+2], value)


def archive_url(rel_url):
    if rel_url and not rel_url.endswith('/'):
        rel_url += '/'
    try:
        response = urllib.request.urlopen(f'{ARCHIVE_URL}/{SITE_URL}/{rel_url}')
        print(response.status, response.reason, rel_url)
    except urllib.error.HTTPError as e:
        print(f'HTTP Error: {e.code} {e.reason} for page {rel_url}')
    except urllib.error.URLError as e:
        print(f'URL Error: {e.reason} for page {rel_url}')


def print_progress(message, func, quiet=False):
    if not quiet:
        print(f'{message}...', end='\r', flush=True)
    output = func()
    if type(output) == tuple:
        output = output[0]
    if not quiet:
        print(f'{message}: {output.strip()}', flush=True)


def remove_build_tree():
    try:
        shutil.rmtree(BUILD_PATH)
    except FileNotFoundError:
        pass
    return f'{BUILD_DIR}/'


def mkdirp(directory):
    try:
        os.mkdir(directory)
    except OSError:
        pass
    return f'{strip_repo_root(directory)}/'


def create_directory(*args):
    directory = os.path.join(BUILD_PATH, *args)
    print_progress('Creating directory', lambda: mkdirp(directory))
    return directory


def process_preamble(src_path):

    content = []
    preamble = None
    parameters = {}

    with open(src_path, 'r') as f:
        for line in f:
            if line == PREAMBLE_SEPARATOR:
                if preamble is None:
                    preamble = True
                else:
                    preamble = False
            elif preamble:
                key, value = line.split(':', 1)
                if key in SPECIAL_KEYS:
                    raise ValueError(f'cannot have key "{key}" in {src_path}')
                parameters[key] = value.strip()
            else:
                content.append(line)

    if LAYOUT_KEY not in parameters:
        raise ValueError(f'must have key "{LAYOUT_KEY}" in {src_path}')

    return content, parameters


def process_parameters(parameters, src_path):

    url_path = strip_repo_root(src_path)
    if os.path.split(url_path)[1] == INDEX_FILE:
        url_path = os.path.split(url_path)[0]
    parameters['url'] = f'{SITE_URL}/{get_name(url_path)}/'
    if strip_repo_root(src_path) == 'about.md':
        parameters['url'] = f'{SITE_URL}/'

    parameters['site_title'] = SITE_TITLE
    parameters['head_title'] = SITE_TITLE
    if 'title' in parameters and parameters['title'] != SITE_TITLE:
        parameters['head_title'] = f'{parameters["title"]} - {SITE_TITLE}'

    if parameters[LAYOUT_KEY] == 'post':
        rel_dir = os.path.relpath(src_path, os.path.join(REPO_ROOT, BLOG_DIR))
        post_date = datetime.datetime.strptime(rel_dir[:4], '%y%m')
        parameters['post_date'] = post_date.strftime('%Y %B')

    return parameters


def process_markdown(content):

    result = []

    for line in content:
        if not line.strip():
            result.append(line)
            continue

        text = line.strip()
        if text.startswith('<') and text.endswith('>'):
            result.append(line)
            continue

        if text.startswith('#'):
            heading, text = text.split(' ', 1)
            text = f'<h{len(heading)}>{text}</h{len(heading)}>'
        elif text.startswith('> '):
            text = f'<q>{text[2:]}</q>'
        else:
            text = f'<p>{text}</p>'

        text = re.sub('\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
        text = re.sub('\*(.*?)\*', r'<em>\1</em>', text)

        indent = line[:len(line) - len(line.lstrip())]
        endl = line[len(line.rstrip()):]
        result.append(indent + text + endl)

    return result


def process_layout(content, name):

    result = []

    with open(os.path.join(REPO_ROOT, LAYOUT_DIR, f'{name}.html'), 'r') as f:
        for line in f:
            if has_tag(line) and get_tag(line) == CONTENT_TAG:
                for contentline in content:
                    indent = line[:len(line) - len(line.lstrip())]
                    result.append(indent + contentline)
            else:
                result.append(line)

    return result


def process_includes(content):

    result = []

    for line in content:
        if has_tag(line) and get_tag(line).startswith(INCLUDE_TAG):
            src_file = get_tag(line).split(' ', 1)[1]
            with open(os.path.join(REPO_ROOT, INCLUDE_DIR, src_file)) as f:
                for includeline in f:
                    indent = line[:len(line) - len(line.lstrip())]
                    result.append(indent + includeline)
        else:
            result.append(line)

    return result


def process_tags(content, parameters):

    result = []

    for line in content:
        while has_tag(line):
            key = get_tag(line)
            if key not in parameters:
                print(f'    Warning: key "{key}" was not found in params')
                line = ''
            else:
                line = replace_tag(line, parameters[key])
        result.append(line)

    return result


def write_page(content, dest_path):

    with open(dest_path, 'w') as f:
        for line in content:
            f.write(line.replace('"%/', f'"{SITE_URL}/'))


def generate_page(src_path, dest_path):

    content, parameters = process_preamble(src_path)
    parameters = process_parameters(parameters, src_path)
    content = process_markdown(content)
    content = process_layout(content, parameters[LAYOUT_KEY])
    content = process_includes(content)
    content = process_tags(content, parameters)
    write_page(content, dest_path)

    return f'{strip_repo_root(src_path)} -> {strip_repo_root(dest_path)}'


def copy_directory(src_dir='', dest_dir=None, quiet=False):

    if dest_dir is None:
        dest_dir = src_dir
    create_directory(dest_dir)

    with os.scandir(os.path.join(REPO_ROOT, src_dir)) as it:
        for entry in it:
            if is_ignored(entry.name):
                continue

            if is_content(entry.name):
                if entry.name == INDEX_FILE:
                    dest_path = os.path.join(BUILD_PATH, dest_dir, INDEX_FILE)
                else:
                    new_dir = create_directory(dest_dir, get_name(entry.name))
                    dest_path = os.path.join(new_dir, INDEX_FILE)

                if entry.is_symlink():
                    real_path = os.path.abspath(os.path.realpath(entry.path))
                    src_path = os.path.join(get_name(real_path), INDEX_FILE)
                    target = os.path.relpath(src_path, src_dir)
                    print_progress('Creating symlink', lambda: (
                            f'{strip_repo_root(dest_path)} -> ' +
                            strip_repo_root(os.path.join(dest_path, target)),
                            os.symlink(target, dest_path)))
                else:
                    print_progress('Generating page', lambda:
                            generate_page(entry.path, dest_path), quiet)

            elif entry.is_dir():
                if os.path.exists(os.path.join(entry.path, '.build')):
                    subprocess.call([
                        'python3',
                        os.path.join(entry.path, SCRIPT_FILE),
                        'build',
                        REPO_ROOT,
                        entry.path,
                    ])
                if os.path.exists(os.path.join(entry.path, '.quiet')):
                    quiet = True

                copy_directory(
                        os.path.join(src_dir, entry.name),
                        os.path.join(dest_dir, entry.name),
                        quiet
                )

            else:
                print_progress('Copying file', lambda: (
                        str(os.path.join(src_dir, entry.name)) + ' -> '
                                + str(os.path.join(dest_dir, entry.name)),
                        shutil.copy(entry.path, os.path.join(
                                BUILD_PATH, dest_dir, entry.name))
                ), quiet)


def crawl(src_dir=''):

    rel_urls = []

    with os.scandir(os.path.join(REPO_ROOT, src_dir)) as it:
        for entry in it:
            if is_ignored(entry.name):
                continue

            if is_content(entry.name):
                rel_url = get_name(strip_repo_root(entry.path))
                if entry.name == INDEX_FILE:
                    rel_url = os.path.split(rel_url)[0]
                rel_urls.append(rel_url)

            elif entry.is_dir():
                if os.path.exists(os.path.join(entry.path, '.archive')):
                    result = subprocess.check_output([
                        'python3',
                        os.path.join(entry.path, SCRIPT_FILE),
                        'archive',
                        REPO_ROOT,
                        entry.path
                    ])
                    rel_urls.extend(result.decode('ascii').split())
                else:
                    rel_urls.extend(crawl(entry.path))

    return rel_urls


def archive():

    print(f'Crawling for URLs...', end='\r', flush=True)
    rel_urls = crawl()
    print(f'Crawling for URLs: {len(rel_urls)} found')
    pool = multiprocessing.dummy.Pool(4)
    for i in range(0, math.ceil(len(rel_urls) / 24)):
        pool.map(archive_url, rel_urls[i*24:i*24+24])
        if i*24 + 24 < len(rel_urls):
            print_progress('Waiting for a minute to avoid HTTP 429 Error',
                lambda: ('done', time.sleep(60)))
    pool.close()
    pool.join()


def build():

    clean()
    copy_directory()
    print_progress('Copying assets directory', lambda: (
            f'{ASSETS_DIR}/ -> {os.path.join(BUILD_DIR, ASSETS_DIR)}/',
            shutil.copytree(
                    os.path.join(REPO_ROOT, ASSETS_DIR),
                    os.path.join(BUILD_PATH, ASSETS_DIR)
            )
    ))


def clean():

    print_progress('Removing build directory', remove_build_tree)

    with os.scandir(REPO_ROOT) as it:
        for entry in it:
            if entry.is_dir():
                if os.path.exists(os.path.join(entry.path, '.clean')):
                    subprocess.call([
                        'python3',
                        os.path.join(entry.path, SCRIPT_FILE),
                        'clean',
                        REPO_ROOT,
                        entry.path
                    ])


def push():

    repo = git.Repo(REPO_ROOT)
    print_progress('Pushing to remote origin',
            lambda: repo.remotes.origin.push()[0].summary)
    print_progress('Pushing subtree to production',
            lambda: repo.git.subtree('push',
                    REPO_REMOTE, BUILD_BRANCH, prefix=BUILD_DIR))


def serve():

    print(f'Serving website at http://localhost:{SERVER_PORT}/')
    server_address = ('', SERVER_PORT)
    request_handler = functools.partial(
            http.server.SimpleHTTPRequestHandler,
            directory=BUILD_PATH
    )
    http.server.HTTPServer(server_address, request_handler).serve_forever()



def bad_input():
    print('Usage: ./web [archive|build|clean|push|serve]')
    exit(1)

if __name__ == '__main__':

    if len(sys.argv) != 2:
        bad_input()

    os.chdir(REPO_ROOT)
    if sys.argv[1] == 'archive':
        archive()
    elif sys.argv[1] == 'build':
        build()
    elif sys.argv[1] == 'clean':
        clean()
    elif sys.argv[1] == 'push':
        push()
    elif sys.argv[1] == 'serve':
        serve()
    else:
        bad_input()
